# LLM Comparison Datasets

A curated list of datasets containing outputs from multiple LLM models on the same prompts/tasks, useful for comparative analysis and evaluation.

## Overview

This document catalogs publicly available datasets that contain responses from multiple language models (ChatGPT, DeepSeek, Claude, Gemini, etc.) to identical prompts or tasks. These datasets are valuable for:
- Comparative model evaluation
- Benchmarking new models
- Understanding model-specific biases and capabilities
- Training judge models or preference models

---

## Hugging Face Datasets

### 1. lmarena-ai/arena-human-preference-55k
**URL**: https://huggingface.co/datasets/lmarena-ai/arena-human-preference-55k

**Description**: Over 55,000 real-world user conversations and preferences across 70+ state-of-the-art LLMs.

**Models Included**: GPT-4, Claude 2, Llama 2, Gemini, Mistral models, and 65+ others

**Format**: Each sample represents a "battle" with:
- Same question/prompt
- Responses from 2 different LLMs (model A and model B)
- User preference label: prefer A, prefer B, tie, or tie (both bad)

**Use Cases**:
- Human preference learning
- Pairwise model comparison
- Training reward models

---

### 2. llm-blender/mix-instruct
**URL**: https://huggingface.co/datasets/llm-blender/mix-instruct

**Description**: 11 responses from popular instruction-following LLMs for the same prompts, with pairwise comparison results from ChatGPT.

**Size**: 4,771 examples in test split

**Models Included**: 11 instruction-following LLMs (specific models not listed)

**Annotations**: Pairwise comparison results generated by prompting ChatGPT

**Use Cases**:
- Instruction-following evaluation
- LLM ensemble training
- Preference ranking research

**Related**: Part of the LLM-Blender project for ensembling LLMs with pairwise ranking

---

### 3. allenai/WildBench & WildBench-V2-Model-Outputs
**URL**:
- Dataset: https://huggingface.co/datasets/allenai/WildBench
- Model Outputs: https://huggingface.co/datasets/allenai/WildBench-V2-Model-Outputs

**Description**: Benchmarking dataset with challenging tasks from real users, ensuring all LLMs are tested on the SAME examples for fair evaluation.

**Size**:
- v1.0: 1,024 examples
- V2: Expanded with additional model outputs

**Models Included**: Multiple verified LLMs (Claude 3.5 Sonnet, GPT-4, and others)

**Key Feature**: Fair evaluation through standardized test set - all models answer identical prompts

**Use Cases**:
- Real-world task evaluation
- Cross-model performance benchmarking
- Wild/diverse task handling

---

### 4. lmsys/chatbot_arena_conversations
**URL**: https://huggingface.co/datasets/lmsys/chatbot_arena_conversations

**Description**: Unrestricted conversations from over 13K users with 20 different LLMs.

**Models Included**: 20 LLMs including GPT-4, Claude-v1, and other strong models

**Source**: Real user interactions in the wild

**Use Cases**:
- Natural conversation evaluation
- User preference analysis
- Diverse task performance comparison

---

### 5. lmsys/lmsys-chat-1m
**URL**: https://huggingface.co/datasets/lmsys/lmsys-chat-1m

**Description**: One million real-world conversations with 25 state-of-the-art LLMs.

**Size**: 1,000,000 conversations

**Models Included**: 25 state-of-the-art LLMs

**Data Fields**:
- Conversation ID
- Model name
- Conversation text (OpenAI API JSON format)
- Detected language tag
- OpenAI moderation API tag

**Use Cases**:
- Large-scale conversation analysis
- Multi-lingual model comparison
- Safety/moderation research

---

### 6. lmsys/mt_bench_human_judgments
**URL**: https://huggingface.co/datasets/lmsys/mt_bench_human_judgments

**Description**: Expert-level pairwise human preferences for model responses to MT-bench questions.

**Size**: 3,300 expert judgments

**Models Included**: 6 models
- GPT-4
- GPT-3.5
- Claude-v1
- Vicuna-13B
- Alpaca-13B
- LLaMA-13B

**Prompts**: 80 MT-bench questions

**Annotation Quality**: Expert-level human judgments

**Use Cases**:
- Human preference alignment
- Expert evaluation benchmarking
- Training preference models

---

### 7. kstevica/llm-comparison
**URL**: https://huggingface.co/datasets/kstevica/llm-comparison

**Description**: Direct comparison dataset for multiple LLM models (details limited from search)

**Use Cases**:
- General LLM comparison
- Model performance analysis

---

## Academic Papers with Datasets

### 8. "A Comparison of DeepSeek and Other LLMs" (February 2025)
**Paper URL**: https://arxiv.org/html/2502.03688v1

**Models Compared**:
- DeepSeek-R1
- GPT-4o-mini
- Gemini-1.5-flash
- Llama-3.1-8b
- Claude-3.5-sonnet

**Tasks**:
- Authorship classification
- Citation classification

**Key Features**:
- Fully-labeled dataset collected by researchers
- Can be used as benchmark for future LLM studies
- Evaluation on classification tasks

**Key Finding**: DeepSeek outperforms Gemini, GPT, and Llama in most cases, but underperforms Claude

---

### 9. "DeepSeek vs. ChatGPT vs. Claude: Scientific Computing Study" (March 2025)
**Paper URL**: https://arxiv.org/html/2502.17764v2
**ScienceDirect**: https://www.sciencedirect.com/science/article/pii/S2095034925000157

**Models Compared**:
- DeepSeek
- ChatGPT
- Claude
- Reasoning-optimized models

**Tasks**:
- Numerical methods
- Physics-informed machine learning
- Operator learning
- Scientific computing problems
- Scientific machine learning

**Use Cases**:
- Scientific domain evaluation
- Computational accuracy assessment
- Domain-specific model selection

---

### 10. "Comparative Analysis: DeepSeek, ChatGPT, and Gemini" (February 2025)
**Paper URL**: https://arxiv.org/html/2503.04783v1

**Models Compared**:
- DeepSeek
- ChatGPT
- Google Gemini

**Methodology**: Performance benchmarking framework with standardized metrics

**Metrics**:
- Accuracy
- Reasoning capability

**Training Data Comparison**:
- DeepSeek: Curated medical, legal, and financial corpora
- ChatGPT: General-purpose web text and code repositories
- Gemini: Multimodal dataset (text, code, visual data)

---

### 11. "Academic Writing Comparison" (2025)
**Paper URL**: https://arxiv.org/pdf/2503.04765

**Models Compared**:
- DeepSeek
- Qwen
- ChatGPT
- Gemini
- Llama
- Mistral
- Gemma

**Evaluation Set**: 40 papers on Digital Twin and Healthcare

**Use Cases**:
- Academic writing quality assessment
- Domain-specific (healthcare/tech) evaluation

---

### 12. MSciNLI: Scientific Natural Language Inference
**Paper URL**: https://arxiv.org/abs/2404.08066
**ACL Anthology**: https://aclanthology.org/2024.naacl-long.90/

**Description**: Multi-domain scientific benchmark for Natural Language Inference

**Size**: 132,320 sentence pairs

**Domains**: 5 scientific domains

**Models Evaluated**:
- Pre-trained Language Models (PLMs)
- Large Language Models (LLMs)

**Performance**:
- Best PLM: 77.21% Macro F1
- Best LLM: 51.77% Macro F1

**Key Feature**: Enables study of domain shift in scientific NLI

**Use Cases**:
- Scientific text understanding
- Cross-domain generalization
- NLI evaluation

---

## Benchmark Platforms & Leaderboards

### 13. Artificial Analysis LLM Leaderboard
**URL**: https://artificialanalysis.ai/leaderboards/models

**Description**: Comparison and ranking of 100+ AI models

**Metrics**:
- Intelligence
- Price
- Performance
- Speed
- Context window

**Models**: OpenAI, Google, DeepSeek, Anthropic, and others

**Use Cases**:
- Model selection
- Cost-performance analysis
- Real-time benchmark tracking

---

### 14. Vellum LLM Leaderboard 2025
**URL**: https://www.vellum.ai/llm-leaderboard

**Description**: Updated leaderboard tracking current LLM performance

**Use Cases**:
- Current model performance tracking
- Comparison across providers

---

### 15. Hugging Face Open LLM Leaderboard
**URL**: https://huggingface.co/blog/open-llm-leaderboard-mmlu

**Description**: Open-source model evaluation and ranking

**Focus**: MMLU and other standardized benchmarks

**Use Cases**:
- Open-source model comparison
- Academic benchmark tracking

---

## Specialized Benchmarks

### 16. MATH Dataset (Challenging Mathematical Problems)
**Description**: 30 challenging mathematical problems used for model comparison

**Models Tested**:
- DeepSeek-R1
- ChatGPT
- Gemini

**Key Finding**: DeepSeek-R1 achieves superior accuracy on complex mathematical problems

**Use Cases**:
- Mathematical reasoning evaluation
- Complex problem-solving assessment

---

### 17. MMLU (Massive Multitask Language Understanding)
**Description**: Knowledge and problem-solving test across academic and professional domains

**Subjects**: 57 different subjects including:
- Elementary mathematics
- US history
- Computer science
- Law
- Scientific disciplines

**Use Cases**:
- Broad knowledge assessment
- Multi-domain capability testing
- Professional domain evaluation

---

## Tools for Creating Comparison Datasets

### Hugging Face AI Sheets
**Description**: Tool to run same dataset through multiple models and compare outputs

**Features**:
- Add columns for each model's output
- Use LLM-as-judge for evaluation
- Side-by-side comparison

**Use Cases**:
- Custom comparison dataset creation
- Interactive model evaluation

---

## Notes on "LNAI" Dataset

**Search Result**: No specific dataset called "LNAI" containing DeepSeek and ChatGPT comparisons across multiple terms was found.

**Possible Interpretations**:
- "LNAI" could refer to "Lecture Notes in Artificial Intelligence" (Springer publication series)
- May be a specific research lab or organization acronym
- Dataset name might be slightly different

**Recommendation**: If you have additional context about the LNAI dataset (such as publication venue, research group, or topic area), it would help narrow down the search.

---

## Dataset Selection Guide

### For Human Preference Research
- **lmarena-ai/arena-human-preference-55k** (55K labeled preferences)
- **lmsys/mt_bench_human_judgments** (expert judgments)

### For Large-Scale Analysis
- **lmsys/lmsys-chat-1m** (1M conversations)
- **lmsys/chatbot_arena_conversations** (13K users, 20 models)

### For Scientific/Technical Domains
- **MSciNLI** (scientific NLI)
- **DeepSeek vs ChatGPT vs Claude paper** (scientific computing)
- **MATH dataset** (mathematical reasoning)

### For Fair Direct Comparison
- **allenai/WildBench** (all models on same examples)
- **llm-blender/mix-instruct** (11 models, same prompts)

### For Multi-Model Ensemble Research
- **llm-blender/mix-instruct** (designed for blending)

---

## Related Resources

### GitHub Repositories
- **tatsu-lab/alpaca_eval**: Automatic evaluator for instruction-following models
- **yuchenlin/LLM-Blender**: Ensembling LLMs with pairwise ranking

### Additional Reading
- Hugging Face LLM Evaluation Guide: https://huggingface.co/blog/clefourrier/llm-evaluation
- LLM-as-a-Judge Cookbook: https://huggingface.co/learn/cookbook/en/llm_judge

---

## Last Updated
October 2025

---

## Transformation Scripts for Turn-Level Analysis

This directory includes Python scripts to transform HuggingFace datasets into turn-level CSV format compatible with FluffyViz.

### Available Scripts

#### 1. `transform_huggingface_dataset.py` - SoftAge-AI Multi-Turn Dataset

**Source**: [SoftAge-AI/multi-turn_dataset](https://huggingface.co/datasets/SoftAge-AI/multi-turn_dataset)

**Status**: ⚠️ Gated dataset - requires HuggingFace token

**Output**: `multi-turn-transformed.csv` (~400 conversations → ~1,600 turns)

**Features**:
- Converts wide format (P1-P5, R1-R4) to long format
- Excludes P5 (final user prompt without response)
- Generates synthetic metadata (tokens, latency, cost)

**Usage**:
```bash
cd sample-data
source .venv/bin/activate
export HF_TOKEN=your_token_here
python3 transform_huggingface_dataset.py
```

#### 2. `transform_wildchat_dataset.py` - Allen AI WildChat Dataset

**Source**: [allenai/wildchat-r1-p2-format-filtered](https://huggingface.co/datasets/allenai/wildchat-r1-p2-format-filtered)

**Status**: ✅ Public dataset - no authentication required

**Output**: `wildchat-transformed.csv` (73,852 conversations → 193,235 turns, 1.1GB)

**Features**:
- Extracts turn-level data from messages array
- Uses conversation_hash as session_id
- Uses hashed_ip as user_id
- Preserves original timestamps and models
- Handles long messages with reasoning traces

**Usage**:
```bash
cd sample-data
source .venv/bin/activate
python3 transform_wildchat_dataset.py
```

### Output Format

Both scripts generate CSVs with these columns:
- `turn_id`: Unique turn identifier
- `session_id`: Conversation identifier
- `user_id`: User identifier
- `timestamp`: ISO 8601 timestamp
- `user_message`: User's prompt
- `assistant_message`: AI response
- `model`: Model name
- `prompt_tokens`: Estimated input tokens
- `completion_tokens`: Estimated output tokens
- `total_tokens`: Total tokens
- `latency_ms`: Simulated latency
- `cost_usd`: Estimated cost

### Virtual Environment Setup

```bash
cd sample-data
python3 -m venv .venv
source .venv/bin/activate
pip install datasets
```

---

## Contributing
If you know of additional datasets comparing multiple LLM outputs, please add them to this document with:
- Dataset name and URL
- Models included
- Size/scale
- Task types
- Key features
